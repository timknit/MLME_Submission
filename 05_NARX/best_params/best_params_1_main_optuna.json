{
  "lag": 5,
  "num_layers": 3,
  "hidden_1": 72,
  "hidden_2": 64,
  "hidden_3": 39,
  "activation": "gelu",
  "batch_size": 128,
  "lr": 0.0002012998313494599,
  "l1_lambda": 1.0795335391966569e-08,
  "l2_lambda": 7.29604113633572e-08,
  "dropout": 0.028346535275646886,
  "loss_function": "huber"
}