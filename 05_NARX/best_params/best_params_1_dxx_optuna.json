{
  "lag": 4,
  "num_layers": 1,
  "hidden_1": 97,
  "activation": "relu",
  "batch_size": 64,
  "lr": 0.0002522126038462181,
  "l1_lambda": 6.857678018642172e-08,
  "l2_lambda": 1.1629403323394067e-09,
  "dropout": 0.011902622428536354,
  "loss_function": "huber"
}