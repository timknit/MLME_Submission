{
  "lag": 5,
  "num_layers": 1,
  "hidden_1": 32,
  "activation": "relu",
  "batch_size": 64,
  "lr": 0.000138879067742347,
  "l1_lambda": 1.3314573336711053e-08,
  "l2_lambda": 1.1582437826333968e-10,
  "dropout": 0.06971852651796423,
  "loss_function": "huber"
}